{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 5    \n",
    "\n",
    "\n",
    "## Introduction\n",
    "Deep learning requires comprehensive knowledge of the building blocks of traditional machine learning.  That includes understanding the difference between supervised and unsupervised learning, which Hyperparameters to tune, and whether to use frequentist estimators or Bayesian inference.   \n",
    "\n",
    "## 5.1 Learning Algorithms     \n",
    "> “A computer\n",
    "program is said to learn from experience E with respect to some class of tasks T\n",
    "and performance measure P , if its performance at tasks in T , as measured by P ,\n",
    "improves with experience E.”\n",
    "> - Mitchell (1997)\n",
    "\n",
    "### 5.1.1 The Task, $T$\n",
    "Machine learning tasks are usually described in terms of how the machine learning system should process an __example__.  An example is a collection of **features** that have been quantitatively measure from some object or event.  An example is a typically a vector __x__ in $R^{n}$.  Using this vector, many problems can be solved by machine learning:\n",
    "- Classification: given $x_{i}$, label y\n",
    "- Classification with missing values: given a partial $x_{i}$, label y\n",
    "- Regression: given $x_{i}$, output a continuous value of y\n",
    "- Transcription: given $x_{i}$, write out a text version of y\n",
    "- Machine Translation: take a $x_{i}$ (typically the origination language) and translate it into $y_{i}$ (the destination language)\n",
    "- Structured Output: take a collection, say $x_{i}$ and map it back to a smaller group $y_{j}$.  Think mapping a collection of words back to noun, adjective, or verbs.  \n",
    "- Anomaly Detection: think misuse of credit cards\n",
    "- Synthesis and sampling: think GAN generation of images or medical claims data\n",
    "- Imputation of Missing Values: given a set $x_{i}$ but with j entries missing, the algorithm must predict what those values are\n",
    "- Denoising: an algorithm is provided a messy $x_{i}$ and it maps it back to a clean $x_{i}$.  \n",
    "- Density Estimation (probability mass function estimation): the algorithm is askes to learn a function that takes a vector __x__ and maps it to a probability density function if continuous and a probability mass function if discrete.  \n",
    "\n",
    "### 5.1.2 The Performance Measue, $P$    \n",
    "For tasks like classification, classification with missing inputs, and transcription, we usually use __accuracy__ to determine a model's performance.  Accuracy is the proportion of examples for which the model produces the correct output.  Additionally, the **error rate**, 1 - Accuracy, is helpful in determining model performance.  For density estimation, these two won't suffice.  The performance metric best used here is determined by reporting the average log-probability the model assigns to examples.  \n",
    "\n",
    "To account for the fact that models will be used in a production setting with data the model hasn't seen before, we split our data into training and test sets.  Then, we calculate the performance metric on that test split.  \n",
    "\n",
    "### 5.1.3 The Experience, $E$\n",
    "\n",
    "Machine learning algorithms can be broadly categorized as __unsupervised__ or **supervised** by what kind of experience they are allowed to have during the learning process.  Most tasks can and will experience the entire dataset when training.  On top of that, models from the realm of reinforcement learning will see data in addition to the whole dataset as they're meant to behave independently.  The factors to train on $x_{i}$ is called a __design matrix__.  Within supervised learning, the design matrix will be trained against a vector of labels, __y__.  \n",
    "\n",
    "### 5.1.4 Linear Regression: an Example\n",
    "Much, if not all, of the code listed below came from [here](https://colab.research.google.com/drive/1HL1HDCDhh1FiV-NckjfTYPd50K7H7wzi#scrollTo=uyKcSFJrbhRf&forceEdit=true&offline=true&sandboxMode=true).  Comments my own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "data = pd.read_csv('data/graduate-admissions/Admission_Predict_Ver1.1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns = [col_name.strip() for col_name in data.columns.tolist()]\n",
    "\n",
    "continuous_features = data[['GRE Score', 'TOEFL Score', 'University Rating', 'SOP', 'LOR', 'CGPA']].values / 100\n",
    "categorical_features = data[['Research']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.concatenate([continuous_features, categorical_features], axis = 1)\n",
    "y = data[['Chance of Admit']].values\n",
    "\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(X, y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.constant(train_features, dtype = tf.float32)\n",
    "y = tf.constant(train_labels, dtype = tf.float32)\n",
    "\n",
    "test_X = tf.constant(test_features, dtype = tf.float32)\n",
    "test_y = tf.constant(test_labels, dtype = tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom performance functions\n",
    "# 1) MSE function\n",
    "# 2) The derivative of the mean squared error function\n",
    "# 3) Hypothesis function / Regression function\n",
    "\n",
    "def mean_squared_error( Y , y_pred ) -> tf.Tensor:\n",
    "    \"\"\"\n",
    "    Args: \n",
    "        Y: list of y's from the data\n",
    "        y_pred: y's predicted from model\n",
    "    \n",
    "    Returns:\n",
    "        The reduced tensor\n",
    "    \"\"\"\n",
    "    return tf.reduce_mean(tf.square(y_pred - Y ))\n",
    "\n",
    "def mean_squared_error_deriv( Y , y_pred ) -> tf.Tensor:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        Y: list of y's from the data\n",
    "        y_pred: y's predicted from model\n",
    "    Return: \n",
    "        The reduced tensor\n",
    "    \"\"\"\n",
    "    return tf.reshape( tf.reduce_mean( 2 * ( y_pred - Y ) ) , [ 1 , 1 ] )\n",
    "\n",
    "def h(X, weights, bias) -> tf.Tensor:\n",
    "    \"\"\" This is our hypothesis function.  It follows the form Y = WX + B which pertain\n",
    "    to the arguments of this function.  \n",
    "    Args: \n",
    "        X: a tensor of X values\n",
    "        weights: a tensor of weights to multiply the tensor of X's by\n",
    "        bias: a tensor to add on to our product\n",
    "    Return:\n",
    "        A tensor\n",
    "    \"\"\"\n",
    "    return tf.tensordot( X , weights , axes=1 ) + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is 11.418600082397461\n",
      "Loss is 5.51230001449585\n",
      "Loss is 2.6654000282287598\n",
      "Loss is 1.2927000522613525\n",
      "Loss is 0.6308000087738037\n",
      "Loss is 0.31209999322891235\n",
      "Loss is 0.15850000083446503\n",
      "Loss is 0.08470000326633453\n",
      "Loss is 0.04919999837875366\n",
      "Loss is 0.032099999487400055\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "num_samples = X.shape[0]\n",
    "batch_size = 10\n",
    "learning_rate = 0.001\n",
    "\n",
    "# create the dataset from the original data\n",
    "dataset = tf.data.Dataset.from_tensor_slices(( X , y))\n",
    "dataset = dataset.shuffle( 500 ).repeat( num_epochs ).batch( batch_size )\n",
    "iterator = dataset.__iter__()\n",
    "\n",
    "num_features = X.shape[1]\n",
    "# initialiaze some random normally distributed weights\n",
    "weights = tf.random.normal( ( num_features , 1 ) ) \n",
    "bias = 0\n",
    "# initialize lists of to display change\n",
    "epochs_plot = list()\n",
    "loss_plot = list()\n",
    "\n",
    "for i in range( num_epochs ) :\n",
    "    epoch_loss = list()\n",
    "    # this will loop 40 times since 400 / 10\n",
    "    for b in range( int(num_samples/batch_size) ):\n",
    "        x_batch , y_batch = iterator.get_next()\n",
    "        # get the predictions from the x-vals times weights plus the biases\n",
    "        output = h( x_batch , weights , bias ) \n",
    "        # add the mean squared error from this iteration onto batch list\n",
    "        loss = epoch_loss.append( mean_squared_error( y_batch , output ).numpy() )\n",
    "\n",
    "        # start updating the weights and biases\n",
    "        ## the first three will find the multiplier by the learning rate to update W\n",
    "        dJ_dH = mean_squared_error_deriv( y_batch , output)\n",
    "        dH_dW = x_batch\n",
    "        dJ_dW = tf.reduce_mean( dJ_dH * dH_dW )\n",
    "        # this identifies what to multiply the learning rate by to update B\n",
    "        dJ_dB = tf.reduce_mean( dJ_dH )\n",
    "\n",
    "        # updates your weights as the learning rate times your \n",
    "        weights -= ( learning_rate * dJ_dW )\n",
    "        bias -= ( learning_rate * dJ_dB ) \n",
    "\n",
    "    # we are optimizing for mean squared error while \n",
    "    loss = np.array( epoch_loss ).mean()\n",
    "    epochs_plot.append( i + 1 )\n",
    "    loss_plot.append( loss ) \n",
    "    print( 'Loss is {}'.format( round(loss, 4) ) ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot( epochs_plot , loss_plot ) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error = 0.13226215541362762\n"
     ]
    }
   ],
   "source": [
    "output = h( test_X , weights , bias ) \n",
    "labels = test_y\n",
    "\n",
    "accuracy_op = tf.metrics.MeanAbsoluteError() \n",
    "accuracy_op.update_state(labels, output)\n",
    "print( 'Mean Absolute Error = {}'.format(accuracy_op.result().numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.2 Capacity, Overfitting, and Underfitting\n",
    "The ability of a model to be trained and subsequently perform well on new inputs is called __generalization__.  To get a sense of expectation of the models performance initially, we can generate the **training error**.  However the creme de la creme of the modeling process revolves around the __test error__.  It is best practice to compare errors of multiple model builds on a __holdout set__.  \n",
    "\n",
    "In general, the field of **statistical learning theory** will shed light onto best practices when trying tomake sure model generalizes well to unseen inputs.  When we split our data into __training__ and **test** sets, we make a couple of assumptions along the way called __i.i.d assumptions__:\n",
    "1) the examples (rows) are **independent** of each other\n",
    "2) the train and test set are __identically distributed__.  \n",
    "Overall, these assumptions help us fit our data into what's called the **data generating distribution** which can be studied and quantified.  Additionally, the two biggest drivers in determining how well a model will generalize to new inputs is:\n",
    "- How well the model minimizes the training error\n",
    "- How small the gap can be made between the training and test error set\n",
    "The factors that shed light onto how well the model is doing in the abover two areas are __overfitting__ and **underfitting**.  The ability of a model to fit many new, unseen inputs is called its __capacity__.  Machine learning models will generally perform best when their capacity matches the true complexity of the task.  \n",
    "\n",
    "## 5.2.1 The \"No Free Lunch Theorem\"\n",
    "Contrary to logic, machine learning hopes to find rules that are _probably_ good for _most_ of the members of the training set.  The **no free lunch theorem** states that, averaged over all possible data generating distributions, every classification algorithm has the same error rate when classifying unobserved points.  Thus, our goal as machine learning researchers should not be to find an optimal model for all scenarios.  Instead, our goal should be to understand which distributions make sense in real life to and fit models with those distributions to the real life scenario.  \n",
    "\n",
    "## 5.2.2 Regularization\n",
    "So far we have just described increasing or decreasing the capacity space of our algorithms when it comes to honing the models so that they generalize well.  Next up, is what is called __regularization__.  To quantify this scenario, let's create a variable measuring **weight decay**.  $J(w) = MSE_{train} + \\lambda w^{T}w$.  Our goal is to minimize mean squared error with consideration for weights vector to have a smaller $L_{2}$ norm.  Our lambda here is a value chosen ahead of time that we will use as to whether we prefer smaller weights.  Underfit models would contain a $\\lambda$ that is too large or excessive.  Overfit models would have a smaller $\\lambda$.  The properly fit models that generalize well to unseen inputs would have a $\\lambda$ in between this.  This process of quantifying the weight decay and optimizing for it is called __regularization__.  There are many regularizers we can add to a problem in order to help the models generalize better.  \n",
    "\n",
    "# 5.3 Hyperparameters\n",
    "Many models have settings within called **hyperparameters**.  To tune these settings, we create a __validation set__ from the training data.  Rule of thumb is to split off the test set and then to split the remaining data into 80% training and 20% validation.  \n",
    "\n",
    "## 5.3.1 Cross Validation\n",
    "When the test set is small, cross validation comes into play.  Cross validation, specfically _k-fold_ cross validation, splits the data into _i_ subsets where a model is trained on _i-1_ subsets and tested on the $i^{th}$ subset.  This is performed for all subsets throughout.  One problem with this is that there is no unbiased estimator for the average error from these subsets but no one seems to care.  \n",
    "\n",
    "# 5.4 Estimators, Bias, and Variance\n",
    "Using statistics to quantify parameter estimation, bias, and variance helps with notating concepts such as generalization, underfitting, and overfitting.  \n",
    "\n",
    "## 5.4.1 Point Estimation\n",
    "Point estimation is the attempt to find the single best prediction of some quantity of interest.  Point estimators are can be chosen as one likes but not all point estimators are created equal and some generalize better than others.  Point estimators are a function of the data values, and with the data values being chose at random, our point estimators are random variables.  From here we will discuss the most studied properties of these point estimators.  \n",
    "\n",
    "## 5.4.2 Bias\n",
    "The bias estimator is defined as:\n",
    "$bias(\\hat{\\theta_{m}}) = E(\\hat{\\theta_{m}}) - \\theta$\n",
    "Thus, it's the difference between the expected value of our point estimator and the true value.  A point estimator $\\hat{\\theta_{m}}$ is said to be **unbiased** if $bias(\\hat{\\theta_{m}})$ = 0 which would imply $E(\\hat{\\theta_{m}}) = \\theta$ based on our equation above. A point estimator $\\hat{\\theta_{m}}$ is said to be __asymptotically unbiased__ if $\\lim_{m\\to\\infty}bias\\hat{\\theta_{m}} = 0$ whic implies $\\lim_{m\\to\\infty}E(\\hat{\\theta_{m})} = \\theta$\n",
    "\n",
    "Cue up three to four examples of unbiased estimators being chosen (like in STA with Bagui).  \n",
    "\n",
    "## 5.4.3 Variance and Standard Error\n",
    "As we have looked at point estimation, there is also an effore we must undertake to look at how estimating this value will vary.  That __variance__ is denoted as $V(\\hat\\theta)$ with the square root of this being called the **standard error** labelled $SE(\\hat\\theta)$.  The $SE(\\hat\\theta) = \\frac{\\sigma}{\\sqrt{m}}$.  Then, the 95% confidence interval centered on the mean $\\hat\\mu_{m}$ is $\\hat\\mu - 1.96SE(\\hat\\mu_{m}), \\hat\\mu_{m} + 1.96SE(\\hat\\mu_{m})$.  \n",
    "\n",
    "## 5.4.4 Trading off Bias and Variance to Minimize Mean Squared Error\n",
    "Bias measures the expected deviation from the true value of the function or parameter.  Variance measures the dispersion from the expected estimator value that any particular sampling of the data is likely to cause.  Would you rather have larger bias?  Or, larger variance?  Using MSE on the estimates wraps Bias and Variance into one equation.  __Consistency__ is the property that ensures that the bias induced by the estimator diminishes as the number of data examples grows.  \n",
    "\n",
    "# 5.5 Maximum Likelihood Estimation\n",
    "Assuming a parametric model of $m$ examples drawn independently from the true but unknown data generating distribution $p_{data}(x)$, we can expect the Maximum Likelihood Estimator (MLE) for $\\theta$ is: $\\theta_{ML} = arg max_{\\theta}p_{model}(X; \\theta) = arg max_{\\theta}\\prod_{i = 1}^{m}p_{model}(x^{i}; \\theta$.  This process of finding the maximum likelihood estimate becomes known as minimizing or maximizing the Kullback-Leibler (KL) divergence.  KL divergence has a minimum value of zero.  \n",
    "\n",
    "## 5.5.1 Conditional Log-likelihood and Mean Square Error\n",
    "Within the context of supervised learning, our goal is to identify the $y$ value that is most likely given our list of x's.  Thus, $\\theta_{ML} = arg max_{\\theta} P(Y | X; \\theta)$.  Or, if i.i.d., $\\theta_{ML} = arg max_{\\theta} \\sum_{i=1}^{m}log P(y^{i} | x^(i); \\theta)$.  "
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

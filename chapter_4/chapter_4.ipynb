{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numerical Computation\n",
    "\n",
    "## Introduction\n",
    "Numerical computate refers to the process of solving mathematical problems by methods that update estimates of solutions by way of an iterative process, rather than analytically deriving a formula which would provide a solution to the problem.  In this space you'd find optimization and solving systems of linear equations.  \n",
    "\n",
    "## **4.1 Overflow and Underflow**    \n",
    "\n",
    "Many of the problems that numerical computation faces is that algorithms derived theoretically don't have always have a one-to-one translation to a computer.  Rounding errors are a great example and can compound across multiple computations.  A form of rounding error is called **underflow** and this is where numbers near zero are rounded to zero.  The opposite is **overflow**.  Overflow occurs when large numbers are approximated as positive or negative infinity.  In the context of deep learning, the softmax function $softmax(x_{i})$\n",
    "\n",
    "\n",
    "## **4.2 Poor Conditioning**    \n",
    "\n",
    "**Conditioning** is defined as the pace at which a function changes w.r.t. small changes in its inputs.  Those that explode on minor tweaks to their inputs are troublesome for scientific computing.  Let's take a look at a function $f(x)$.  When A is in the Real space and has eigenvalue decomposition, we say its **condition number** is: $max_{i,j}|\\frac{\\lambda_{i}}{\\lambda_{j}}|$    \n",
    "\n",
    "\n",
    " This value is the ratio of the magnitude of the largest and smallest eigenvalues.  You can expect that when this ratio is large, the matrix inversion is _very_ sensitive to error in the input.  \n",
    "\n",
    "## **4.3 Gradient-Based Optimization**    \n",
    "\n",
    "**Optimization** is apart of many deep learning operations.  You use it to minimize or maximize a function we call the **objective function**.  There are other names for it like: **cost function**, **loss function**, or **error function**.  Throughout this text the value minimizing or maximizing a function will be denoted with a superscript asterisk.  Thus, \\textbf{x}$^{*}$ = arg min $f(x)$.  The most popular form of optimizing the objective function is by using **gradient descent**.  This technique takes small steps away from x with opposite signs of the derivative in order to find the global minimum in $f(x)$.  However, gradient descent itself doesn't guarantee that global min/max will be found.  Instead, we may just find a local min/max or a saddle point in the process.  The **directional derivative** is the derivative of the function $f($__x__ + $\\alpha$**u**$)$ with respect to $\\alpha$, evaluated at $\\alpha = 0$.  \n",
    "\n",
    "Using the chain rule from calculus, $u^{T}\\nabla_{x}f(x)$ is the derivative with respect to $\\alpha$ when $\\alpha = 0$.  To minimize this we find that $x^{'} = x - \\epsilon\\nabla_{x}f(x)$.  This process of subtracting the product of $\\epsilon$, $\\nabla$, and the original function from the array of x's is called the **method of steepest descent** or __gradient descent__.     \n",
    "\n",
    "$\\epsilon$ is what we call the **learning rate**.  The learning rate can be thought of as how far you would like to step from your original point with each iteration.  Optimally, the best learning rate is found by trying many values.  By rule of thumb, though, the smaller the better.  That is within reason: too small a learning rate and your model will take forever to converge.  Make it too big and the gradient descent algorithm will spring from its boundaries making your overall movement larger and larger with each iteration.  \n",
    "\n",
    "## 4.3.1 Jacobian and Hessian Matrix Additions to the Gradient    \n",
    "A matrix that contains all partial derivated of a function whose input and output are both vectors is valled a **Jacobian matrix**.  The official definition is that if we have a function $f: R^{m} -> R^{n}$, then the JacobAnywhian matrix __J__ $\\in R^{n x m}$ of $f$ is defined s.t. $J_{i,j} = \\frac{\\partial }{\\partial x_{j}} f(x)_{i}$.  From this point, it behooves us to take the second derivative to indicate the measure of **curvature**.  To expound, consider a quadratice function which we take two derivatives of.  That would leave us with 0 value second derivative.  This would indicate _zero_ curvature.  That second derivative could be calculated using only the gradient.  If our gradient equals 1, then we step out $\\epsilon$ on our negative gradient and our objective function will decrease by $\\epsilon$.  If non-zero and negative, the function curves downward and the cost function will decrease more than $\\epsilon$.  If non-zero and positive, then it would decrease by less than $\\epsilon$.  Though text may be beneficial, the figure provided in the text does this scenario justice:\n",
    "![Figure 4.4: Various Curvatures](https://github.com/nschampions2004/deep_learning_notes/blob/master/chapter_4/images/figure_4.4_various_curvatures.png)\n",
    "\n",
    "The Jacobian is centered around a singular input.  When we expand to multiple inputs, we use what's called a __Hessian matrix__.  It's notation is **H**(f)(__x__) is defined by:  **H**(f)(__x__)$_{i,j} = \\frac{\\partial^{2}}{\\partial x_{i}\\partial x_{j}}f(x)$.  Connecting the Jacobian, the Hessian is the Jacobian of the gradient.  \n",
    "\n",
    "Anywhere that the second partial derivatives are continuous, the differential operators are commutative, i.e. their order can be swapped: so differentiating $f($__x__$)$ w.r.t $x_{i}$ and then $x_{j}$ will yield the same result as differentiating $f($**x**$)$ first with $x_{j}$ and then $x_{i}$.  With the Hessian matrix being real and symmetric, we can do some matrix decomposition into a set of eigenvalues and an orthogonal basis.    \n",
    "\n",
    "\n",
    "\n",
    "Pg. 87 - Pg. 93.5    \n",
    "<!THERE NEEDS TO BE MORE HERE BUT I JUST CANT \n",
    "\n",
    "## 4.4 Constrained Optimization\n",
    "There are times where we don't want to find the max/minimum value over all possible sets of x's.  Instead within a set of x's, $S$, and min and max can be found.  This process is known as **constrained optimization**.  One of the solutions to solving these constrained optimization problems is to make $||x|| \\leq 1$.  A popular way of working with the constraints is to use gradient descent to take the contraint into account.  From there we take small $\\epsilon$ steps towards our goal and, then, project the result back into $S$.  If we use a line search, we can search only over step sizes $\\epsilon$ that produce valid values of __x__ or we can project each point on that line back into the constraint region.     \n",
    "\n",
    "<MORE HERE BUT MY MIND EXPLODED "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
